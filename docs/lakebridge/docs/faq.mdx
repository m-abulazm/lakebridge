---
sidebar_position: 8
---
# FAQs

## [Table of Contents](#table-of-contents)
* [Installation](#installation)
* [Reconcile](#reconcile)

----

## Installation
### <u> Install Databricks CLI on Linux without brew</u>
    ```shell
    #!/usr/bin/env bash

    #install dependencies
    apt update && apt install -y curl sudo unzip

    #install databricks cli
    curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/v0.242.0/install.sh | sudo sh
    ```
----
## Reconcile
### <u>Guidance for Oracle as a source</u>

### Driver

#### Option 1
import CodeBlock from '@theme/CodeBlock';

* **Download `ojdbc8.jar` from Oracle:**
Visit the [official Oracle website](https://www.oracle.com/database/technologies/appdev/jdbc-downloads.html) to
acquire the `ojdbc8.jar` JAR file. This file is crucial for establishing connectivity between Databricks and Oracle
databases.

* **Install the JAR file on Databricks:**
Upon completing the download, install the JAR file onto your Databricks cluster. Refer
to [this page](https://docs.databricks.com/en/libraries/cluster-libraries.html)
For comprehensive instructions on uploading a JAR file, Python egg, or Python wheel to your Databricks workspace.

#### Option 2

* **Install ojdbc8 library from Maven:**
Follow [this guide](https://docs.databricks.com/en/libraries/package-repositories.html#maven-or-spark-package) to
install the Maven library on a cluster. Refer
to [this document](https://mvnrepository.com/artifact/com.oracle.database.jdbc/ojdbc8) for obtaining the Maven
coordinates.

This installation is a necessary step to enable seamless comparison between Oracle and Databricks, ensuring that the
required Oracle JDBC functionality is readily available within the Databricks environment.


### <u>Commonly Used Custom Transformations</u>

| source_type | data_type     | source_transformation                                                                                        | target_transformation                                                                 | source_value_example    | target_value_example    | comments                                                                                    |
|-------------|---------------|--------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|-------------------------|-------------------------|---------------------------------------------------------------------------------------------|
| Oracle      | number(10,5)  | <CodeBlock language="sql">"trim(to_char(coalesce(col_name,0.0), ’99990.99999’))"</CodeBlock>                 | <CodeBlock language="sql">"cast(coalesce(col_name,0.0) as decimal(10,5))"</CodeBlock> | 1.00                    | 1.00000                 | this can be used for any precision and scale by adjusting accordingly in the transformation |
| Snowflake   | array         | <CodeBlock language="sql">"array_to_string(array_compact(col_name),’,’)"</CodeBlock>                         | <CodeBlock language="sql">"concat_ws(’,’, col_name)"</CodeBlock>                      | [1,undefined,2]         | [1,2]                   | in case of removing "undefined" during migration(converts sparse array to dense array)      |
| Snowflake   | array         | <CodeBlock language="sql">"array_to_string(array_sort(array_compact(col_name), true, true),’,’)"</CodeBlock> | <CodeBlock language="sql">"concat_ws(’,’, col_name)"</CodeBlock>                      | [2,undefined,1]         | [1,2]                   | in case of removing "undefined" during migration and want to sort the array                 |
| Snowflake   | timestamp_ntz | <CodeBlock language="sql">"date_part(epoch_second,col_name)"</CodeBlock>                                     | <CodeBlock language="sql">"unix_timestamp(col_name)"</CodeBlock>                      | 2020-01-01 00:00:00.000 | 2020-01-01 00:00:00.000 | convert timestamp_ntz to epoch for getting a match between Snowflake and data bricks        |


